{
 "cells":[
  {
   "cell_type":"code",
   "source":[
    "\n",
    "\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import nltk"
   ],
   "execution_count":12,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"jF22Ioqv17mJHmbvyeVT2E",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"vader_lexicon\")\n",
    "\"\"\"\n",
    "things to download on the fly if not using py ide:\n",
    "stopwords\n",
    "punkt\n",
    "vader_lexicon\n",
    "\n",
    "\"\"\""
   ],
   "execution_count":13,
   "outputs":[
    {
     "name":"stderr",
     "text":[
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     \/home\/datalore\/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to \/home\/datalore\/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     \/home\/datalore\/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ],
     "output_type":"stream"
    },
    {
     "data":{
      "text\/plain":[
       "'\\nthings to download on the fly if not using py ide:\\nstopwords\\npunkt\\nvader_lexicon\\n\\n'"
      ]
     },
     "metadata":{
      
     },
     "output_type":"display_data"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"cdRCb7ZPE8lq0k7yDkXc5n",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "# nltk.download()\n",
    "\n",
    "sentiment_name = \"Sentiment\"\n",
    "text_col_name = \"Text\"\n",
    "subjectivity_label_name = \"subjectivity\"\n",
    "polarity_label_name = \"polarity\"\n",
    "token_col_name = \"Raw tokens\"\n",
    "tokenized_col_name = \"Tokenized\"\n",
    "length_col_name = \"Token length\"\n",
    "ref_sentiment_name = \"NLTK ref sentiment\"\n",
    "\n",
    "\n",
    "use_csv_col_as_idx = False\n",
    "data_path = \"biden_tweets_labeled.csv\"\n",
    "columns_to_read = [text_col_name, subjectivity_label_name, polarity_label_name]\n",
    "\n",
    "unk_word_name = \"unknown word\"\n",
    "unknown_word_id = -1\n",
    "\n",
    "remove_at_tags_in_tweets = True\n",
    "label_map_dict = {2:2,1:1,0:0}\n",
    "\n",
    "truncate_length = 50\n",
    "truncate_to_unknown_corpus_length_limit = 5000\n",
    "\n",
    "append_nltk_reference = True\n",
    "pad_features = True"
   ],
   "execution_count":14,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"lVIufGFkM6x3utDiVMf0HA",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "# tweets_csv = pd.read_csv(data_path)\n",
    "# if tweets_csv.columns[0] == \"Unnamed: 0\":\n",
    "print(f\"reading data from path {data_path}\")\n",
    "if use_csv_col_as_idx:\n",
    "    print(f\"first column as index, reading csv\")\n",
    "    tweets_csv = pd.read_csv(data_path, index_col=[0])\n",
    "else:\n",
    "    print(f\"first column is named, fall back to specify used_cols\")\n",
    "    tweets_csv = pd.read_csv(data_path, usecols=columns_to_read)"
   ],
   "execution_count":15,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "first column is named, fall back to specify used_cols\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"DisMlPEZj5coqjmx2Hg2JR",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "overall_tokens = []\n",
    "\n",
    "\"\"\"\n",
    "punct to replace: \n",
    "’ to '\n",
    "` to '\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def remove_at_tags(x: pd.Series):\n",
    "    x[text_col_name]: str\n",
    "    words = x[text_col_name].split()\n",
    "    for idx in range(len(words)):\n",
    "        words[idx] = words[idx].replace(\"’\", \"'\")\n",
    "        words[idx] = words[idx].replace(\"`\", \"'\")\n",
    "    \n",
    "    words = [x if not re.match(r\"https?:\", x) else \"website_name\" for x in words]\n",
    "    words_w_at_tags = [x for x in words if not re.match(r\".*@.*\", x)]\n",
    "\n",
    "    result = ''\n",
    "    for elem in words_w_at_tags:\n",
    "        result += elem + ' '\n",
    "    return result\n",
    "\n",
    "\n",
    "def tweet_en_tokenize(x: pd.Series):\n",
    "    global overall_tokens\n",
    "    tokens = word_tokenize(x[text_col_name])\n",
    "    tokens_w_stops = [x for x in tokens if x not in stopwords]\n",
    "    overall_tokens += tokens_w_stops\n",
    "    return tokens_w_stops\n",
    "\n",
    "\n",
    "def apply_self_mapping_of_label(x: pd.Series):\n",
    "    return label_map_dict[x[polarity_label_name]]\n",
    "\n",
    "if remove_at_tags_in_tweets:\n",
    "    tweets_csv[text_col_name] = tweets_csv.apply(remove_at_tags, axis=1)\n",
    "\n",
    "tweets_csv[token_col_name] = tweets_csv.apply(tweet_en_tokenize, axis=1)\n",
    "\n",
    "tweets_csv[polarity_label_name] = tweets_csv.apply(apply_self_mapping_of_label, axis=1)\n",
    "\n",
    "tweets_csv[length_col_name] = tweets_csv.apply(lambda x: len(x[token_col_name]), axis=1)\n",
    "\n",
    "tweets_csv = tweets_csv[tweets_csv[length_col_name] <= truncate_length]"
   ],
   "execution_count":17,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"6hMLnmUggJ6kOQkH0XdKyA",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "tweet_freq_dict = nltk.FreqDist(overall_tokens)\n",
    "print(type(tweet_freq_dict))\n",
    "tweet_freq_dict.tabulate(25)"
   ],
   "execution_count":18,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "<class 'nltk.probability.FreqDist'>\n",
      "           .        Biden            ,            !           's website_name            I            ?          n't        Trump            #          Joe            :          The    President        would          ...            ;     election       people         like            &          100          amp           '' \n",
      "        2042         1660         1083          658          588          531          426          410          349          319          301          292          176          160          143          137          137          128          127          115          108          107          101          100           99 \n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"6Mq3bSy1ti9jaYAFtDvDLV",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "vocab_to_int_encoding = {pair[1]:pair[0]+1 for pair in enumerate(tweet_freq_dict)}\n",
    "# print(len(vocab_to_int_encoding))\n",
    "# print(type(vocab_to_int_encoding))"
   ],
   "execution_count":19,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"m5uJcYi3OTa5cpuqAQTAF0",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "\n",
    "assert truncate_to_unknown_corpus_length_limit < len(vocab_to_int_encoding), \"unknown truncation limit must be smaller than corpus length\"\n",
    "vocab_to_int_encoding[\"<unk>\"] = truncate_to_unknown_corpus_length_limit + 1\n",
    "\n",
    "def tokens_to_int(x: pd.Series):\n",
    "    tokens = x[token_col_name]\n",
    "    try:\n",
    "        tokens_in_int = [vocab_to_int_encoding[token] for token in tokens]\n",
    "        for idx in range(len(tokens_in_int)):\n",
    "            if tokens_in_int[idx] >= truncate_to_unknown_corpus_length_limit:\n",
    "                tokens_in_int[idx] = truncate_to_unknown_corpus_length_limit+1\n",
    "    except KeyError:\n",
    "        print(x)\n",
    "        return -1\n",
    "    return tokens_in_int\n",
    "\n",
    "tweets_csv[tokenized_col_name] = tweets_csv.apply(tokens_to_int, axis=1)\n"
   ],
   "execution_count":20,
   "outputs":[
    {
     "name":"stderr",
     "text":[
      "<ipython-input-20-a877375047c9>:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https:\/\/pandas.pydata.org\/pandas-docs\/stable\/user_guide\/indexing.html#returning-a-view-versus-a-copy\n",
      "  tweets_csv[tokenized_col_name] = tweets_csv.apply(tokens_to_int, axis=1)\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"NBo8UjC1HT0zBeL1d7VVKK",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "if append_nltk_reference:\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "\n",
    "    def tweet_find_nltk_polarity(x: pd.Series):\n",
    "        senti = sia.polarity_scores(x[text_col_name])\n",
    "        return senti['compound']\n",
    "\n",
    "\n",
    "    tweets_csv[ref_sentiment_name] = tweets_csv.apply(tweet_find_nltk_polarity, axis=1)"
   ],
   "execution_count":21,
   "outputs":[
    {
     "name":"stderr",
     "text":[
      "<ipython-input-21-f3adc6fcf0ce>:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https:\/\/pandas.pydata.org\/pandas-docs\/stable\/user_guide\/indexing.html#returning-a-view-versus-a-copy\n",
      "  tweets_csv[ref_sentiment_name] = tweets_csv.apply(tweet_find_nltk_polarity, axis=1)\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"uCNH4xe2QVlpemnkDNj49v",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "if pad_features:\n",
    "    # pad features\n",
    "    def pad_tokens(x: pd.Series):\n",
    "        tokens = x[tokenized_col_name]\n",
    "        padding = [0] * (truncate_length - len(tokens))\n",
    "        return padding + tokens\n",
    "\n",
    "    tweets_csv.loc[:, tokenized_col_name] = tweets_csv.apply(pad_tokens, axis=1)\n",
    "tweets_csv"
   ],
   "execution_count":22,
   "outputs":[
    {
     "name":"stderr",
     "text":[
      "\/opt\/python\/envs\/default\/lib\/python3.8\/site-packages\/pandas\/core\/indexing.py:1773: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https:\/\/pandas.pydata.org\/pandas-docs\/stable\/user_guide\/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(ilocs[0], value, pi)\n"
     ],
     "output_type":"stream"
    },
    {
     "data":{
      "text\/html":[
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "<\/style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th><\/th>\n",
       "      <th>Text<\/th>\n",
       "      <th>subjectivity<\/th>\n",
       "      <th>polarity<\/th>\n",
       "      <th>Raw tokens<\/th>\n",
       "      <th>Token length<\/th>\n",
       "      <th>Tokenized<\/th>\n",
       "      <th>NLTK ref sentiment<\/th>\n",
       "    <\/tr>\n",
       "  <\/thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0<\/th>\n",
       "      <td>That's the guy who is funding those fake stori...<\/td>\n",
       "      <td>1<\/td>\n",
       "      <td>1<\/td>\n",
       "      <td>[That, 's, guy, funding, fake, stories, Hunter...<\/td>\n",
       "      <td>9<\/td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...<\/td>\n",
       "      <td>-0.4767<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>1<\/th>\n",
       "      <td>Biden apparently just told JTaps that he's goi...<\/td>\n",
       "      <td>1<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>[Biden, apparently, told, JTaps, 's, going, as...<\/td>\n",
       "      <td>22<\/td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...<\/td>\n",
       "      <td>0.2732<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>2<\/th>\n",
       "      <td>They've been given 40 chances. And have blown ...<\/td>\n",
       "      <td>1<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>[They, 've, given, 40, chances, ., And, blown,...<\/td>\n",
       "      <td>40<\/td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 73, 111, 1743, ...<\/td>\n",
       "      <td>0.8442<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>3<\/th>\n",
       "      <td>They could not raise the money to beat Biden b...<\/td>\n",
       "      <td>1<\/td>\n",
       "      <td>1<\/td>\n",
       "      <td>[They, could, raise, money, beat, Biden, elect...<\/td>\n",
       "      <td>23<\/td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...<\/td>\n",
       "      <td>0.5367<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>4<\/th>\n",
       "      <td>Can't Biden just fire the board members on the...<\/td>\n",
       "      <td>1<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>[Ca, n't, Biden, fire, board, members, postal,...<\/td>\n",
       "      <td>11<\/td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...<\/td>\n",
       "      <td>0.2584<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>...<\/th>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>1761<\/th>\n",
       "      <td>That's nice, but I hope Biden doesn't think #M...<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>2<\/td>\n",
       "      <td>[That, 's, nice, ,, I, hope, Biden, n't, think...<\/td>\n",
       "      <td>12<\/td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...<\/td>\n",
       "      <td>0.6956<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>1762<\/th>\n",
       "      <td>OMG. You are a sensitive soul. For the record ...<\/td>\n",
       "      <td>1<\/td>\n",
       "      <td>1<\/td>\n",
       "      <td>[OMG, ., You, sensitive, soul, ., For, record,...<\/td>\n",
       "      <td>38<\/td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1259, 1, ...<\/td>\n",
       "      <td>0.9001<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>1763<\/th>\n",
       "      <td>No, IQ45 is trying to steal the election from ...<\/td>\n",
       "      <td>1<\/td>\n",
       "      <td>1<\/td>\n",
       "      <td>[No, ,, IQ45, trying, steal, election, Biden, ...<\/td>\n",
       "      <td>25<\/td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...<\/td>\n",
       "      <td>-0.8074<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>1764<\/th>\n",
       "      <td>Hillary just didn't cheat enough last time. Th...<\/td>\n",
       "      <td>1<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>[Hillary, n't, cheat, enough, last, time, ., T...<\/td>\n",
       "      <td>33<\/td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...<\/td>\n",
       "      <td>-0.5659<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>1765<\/th>\n",
       "      <td>Trump has 70 million supporters... Biden has 8...<\/td>\n",
       "      <td>1<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>[Trump, 70, million, supporters, ..., Biden, 8...<\/td>\n",
       "      <td>22<\/td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...<\/td>\n",
       "      <td>-0.8885<\/td>\n",
       "    <\/tr>\n",
       "  <\/tbody>\n",
       "<\/table>\n",
       "<p>1758 rows × 7 columns<\/p>\n",
       "<\/div>"
      ]
     },
     "metadata":{
      
     },
     "output_type":"display_data"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"xmMHBEmqwnty4hZYgriQ25",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    
   ],
   "execution_count":null,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"wUnT630KODLi67xhgF5tax",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  }
 ],
 "metadata":{
  "kernelspec":{
   "display_name":"Python",
   "language":"python",
   "name":"python"
  },
  "datalore":{
   "version":1,
   "computation_mode":"JUPYTER",
   "package_manager":"pip",
   "base_environment":"default",
   "packages":[
    {
     "name":"nltk",
     "version":"3.7",
     "source":"PIP"
    }
   ]
  }
 },
 "nbformat":4,
 "nbformat_minor":4
}