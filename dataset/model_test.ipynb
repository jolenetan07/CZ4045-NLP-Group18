{
 "cells":[
  {
   "cell_type":"code",
   "source":[
    "import string\n",
    "\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import re\n",
    "\n",
    "import nltk"
   ],
   "execution_count":1,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"jF22Ioqv17mJHmbvyeVT2E",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"vader_lexicon\")\n",
    "\"\"\"\n",
    "things to download on the fly if not using py ide:\n",
    "stopwords\n",
    "punkt\n",
    "vader_lexicon\n",
    "\n",
    "\"\"\""
   ],
   "execution_count":2,
   "outputs":[
    {
     "name":"stderr",
     "text":[
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     \/home\/datalore\/nltk_data...\n",
      "[nltk_data]   Unzipping corpora\/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to \/home\/datalore\/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\/punkt.zip.\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     \/home\/datalore\/nltk_data...\n"
     ],
     "output_type":"stream"
    },
    {
     "data":{
      "text\/plain":[
       "'\\nthings to download on the fly if not using py ide:\\nstopwords\\npunkt\\nvader_lexicon\\n\\n'"
      ]
     },
     "metadata":{
      
     },
     "output_type":"display_data"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"cdRCb7ZPE8lq0k7yDkXc5n",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "# nltk.download()\n",
    "\n",
    "sentiment_name = \"Sentiment\"\n",
    "text_col_name = \"Text\"\n",
    "subjectivity_label_name = \"subjectivity\"\n",
    "polarity_label_name = \"polarity\"\n",
    "token_col_name = \"Raw tokens\"\n",
    "tokenized_col_name = \"Tokenized\"\n",
    "length_col_name = \"Token length\"\n",
    "ref_sentiment_name = \"NLTK ref sentiment\"\n",
    "\n",
    "columns_to_read = [text_col_name, subjectivity_label_name, polarity_label_name]\n",
    "\n",
    "unk_word_name = \"unknown word\"\n",
    "unknown_word_id = -1\n",
    "\n",
    "truncate_length = 50\n",
    "\n",
    "use_csv_col_as_idx = False\n",
    "data_path = \"biden_tweets_labeled.csv\""
   ],
   "execution_count":3,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"lVIufGFkM6x3utDiVMf0HA",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "# tweets_csv = pd.read_csv(data_path)\n",
    "# if tweets_csv.columns[0] == \"Unnamed: 0\":\n",
    "if use_csv_col_as_idx:\n",
    "    print(f\"first column as index, reading csv\")\n",
    "    tweets_csv = pd.read_csv(data_path, index_col=[0])\n",
    "else:\n",
    "    print(f\"first column is named, fall back to specify used_cols\")\n",
    "    tweets_csv = pd.read_csv(data_path, usecols=columns_to_read)"
   ],
   "execution_count":4,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "first column is named, fall back to specify used_cols\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"DisMlPEZj5coqjmx2Hg2JR",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "tweets_csv"
   ],
   "execution_count":5,
   "outputs":[
    {
     "data":{
      "text\/html":[
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "<\/style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th><\/th>\n",
       "      <th>Text<\/th>\n",
       "      <th>subjectivity<\/th>\n",
       "      <th>polarity<\/th>\n",
       "    <\/tr>\n",
       "  <\/thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0<\/th>\n",
       "      <td>@RT_com That‚Äôs the guy who is funding those fa...<\/td>\n",
       "      <td>1<\/td>\n",
       "      <td>1<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>1<\/th>\n",
       "      <td>Biden apparently just told JTaps that he's goi...<\/td>\n",
       "      <td>1<\/td>\n",
       "      <td>0<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>2<\/th>\n",
       "      <td>@Kingofgameplay1 @HeathMayo They've been given...<\/td>\n",
       "      <td>1<\/td>\n",
       "      <td>0<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>3<\/th>\n",
       "      <td>@conorjrogers @reedgalen They could not raise ...<\/td>\n",
       "      <td>1<\/td>\n",
       "      <td>1<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>4<\/th>\n",
       "      <td>Can`t Biden just fire the board members on the...<\/td>\n",
       "      <td>1<\/td>\n",
       "      <td>0<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>...<\/th>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>1761<\/th>\n",
       "      <td>@KThomasDC @costareports That‚Äôs nice, but I ho...<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>2<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>1762<\/th>\n",
       "      <td>@livingdead1970 OMG.  You are a sensitive soul...<\/td>\n",
       "      <td>1<\/td>\n",
       "      <td>1<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>1763<\/th>\n",
       "      <td>@bryceagen @itsJeffTiedrich @realDonaldTrump @...<\/td>\n",
       "      <td>1<\/td>\n",
       "      <td>1<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>1764<\/th>\n",
       "      <td>@Tomboliko @the_resistor @realDonaldTrump Hill...<\/td>\n",
       "      <td>1<\/td>\n",
       "      <td>0<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>1765<\/th>\n",
       "      <td>@Nix3112 @vvsdagger @childishhippy @BigBullyZe...<\/td>\n",
       "      <td>1<\/td>\n",
       "      <td>0<\/td>\n",
       "    <\/tr>\n",
       "  <\/tbody>\n",
       "<\/table>\n",
       "<p>1766 rows √ó 3 columns<\/p>\n",
       "<\/div>"
      ]
     },
     "metadata":{
      
     },
     "output_type":"display_data"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"exZXpT0fokGS1AoLOcSr7j",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "len(tweets_csv)"
   ],
   "execution_count":6,
   "outputs":[
    {
     "data":{
      "text\/plain":[
       "1766"
      ]
     },
     "metadata":{
      
     },
     "output_type":"display_data"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"p5hGMxzCMlT7D4u0mqQ75w",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "overall_tokens = []\n",
    "\n",
    "\"\"\"\n",
    "punct to replace: \n",
    "‚Äô to '\n",
    "` to '\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def remove_at_tags(x: pd.Series):\n",
    "    x[text_col_name]: str\n",
    "    words = x[text_col_name].split()\n",
    "    for idx in range(len(words)):\n",
    "        words[idx] = words[idx].replace(\"‚Äô\", \"'\")\n",
    "        words[idx] = words[idx].replace(\"`\", \"'\")\n",
    "    \n",
    "    words = [x if not re.match(r\"https?:\", x) else \"website_name\" for x in words]\n",
    "    words_w_at_tags = [x for x in words if not re.match(r\".*@.*\", x)]\n",
    "\n",
    "    result = ''\n",
    "    for elem in words_w_at_tags:\n",
    "        result += elem + ' '\n",
    "    return result\n",
    "\n",
    "\n",
    "def tweet_en_tokenize(x: pd.Series):\n",
    "    global overall_tokens\n",
    "    tokens = word_tokenize(x[text_col_name])\n",
    "    tokens_w_stops = [x for x in tokens if x not in stopwords]\n",
    "    overall_tokens += tokens_w_stops\n",
    "    return tokens_w_stops\n",
    "\n",
    "label_map_dict = {2:0.5,1:1,0:0}\n",
    "def apply_self_mapping_of_label(x: pd.Series):\n",
    "    return label_map_dict[x[polarity_label_name]]\n",
    "\n",
    "tweets_csv[text_col_name] = tweets_csv.apply(remove_at_tags, axis=1)\n",
    "\n",
    "tweets_csv[token_col_name] = tweets_csv.apply(tweet_en_tokenize, axis=1)\n",
    "\n",
    "tweets_csv[polarity_label_name] = tweets_csv.apply(apply_self_mapping_of_label, axis=1)\n",
    "\n",
    "tweets_csv[length_col_name] = tweets_csv.apply(lambda x: len(x[token_col_name]), axis=1)\n",
    "\n",
    "tweets_csv = tweets_csv[tweets_csv[length_col_name] <= 50]\n",
    "\n",
    "tweets_csv"
   ],
   "execution_count":7,
   "outputs":[
    {
     "data":{
      "text\/html":[
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "<\/style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th><\/th>\n",
       "      <th>Text<\/th>\n",
       "      <th>subjectivity<\/th>\n",
       "      <th>polarity<\/th>\n",
       "      <th>Raw tokens<\/th>\n",
       "      <th>Token length<\/th>\n",
       "    <\/tr>\n",
       "  <\/thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0<\/th>\n",
       "      <td>That's the guy who is funding those fake stori...<\/td>\n",
       "      <td>1<\/td>\n",
       "      <td>1.0<\/td>\n",
       "      <td>[That, 's, guy, funding, fake, stories, Hunter...<\/td>\n",
       "      <td>9<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>1<\/th>\n",
       "      <td>Biden apparently just told JTaps that he's goi...<\/td>\n",
       "      <td>1<\/td>\n",
       "      <td>0.0<\/td>\n",
       "      <td>[Biden, apparently, told, JTaps, 's, going, as...<\/td>\n",
       "      <td>22<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>2<\/th>\n",
       "      <td>They've been given 40 chances. And have blown ...<\/td>\n",
       "      <td>1<\/td>\n",
       "      <td>0.0<\/td>\n",
       "      <td>[They, 've, given, 40, chances, ., And, blown,...<\/td>\n",
       "      <td>40<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>3<\/th>\n",
       "      <td>They could not raise the money to beat Biden b...<\/td>\n",
       "      <td>1<\/td>\n",
       "      <td>1.0<\/td>\n",
       "      <td>[They, could, raise, money, beat, Biden, elect...<\/td>\n",
       "      <td>23<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>4<\/th>\n",
       "      <td>Can't Biden just fire the board members on the...<\/td>\n",
       "      <td>1<\/td>\n",
       "      <td>0.0<\/td>\n",
       "      <td>[Ca, n't, Biden, fire, board, members, postal,...<\/td>\n",
       "      <td>11<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>...<\/th>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>1761<\/th>\n",
       "      <td>That's nice, but I hope Biden doesn't think #M...<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>0.5<\/td>\n",
       "      <td>[That, 's, nice, ,, I, hope, Biden, n't, think...<\/td>\n",
       "      <td>12<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>1762<\/th>\n",
       "      <td>OMG. You are a sensitive soul. For the record ...<\/td>\n",
       "      <td>1<\/td>\n",
       "      <td>1.0<\/td>\n",
       "      <td>[OMG, ., You, sensitive, soul, ., For, record,...<\/td>\n",
       "      <td>38<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>1763<\/th>\n",
       "      <td>No, IQ45 is trying to steal the election from ...<\/td>\n",
       "      <td>1<\/td>\n",
       "      <td>1.0<\/td>\n",
       "      <td>[No, ,, IQ45, trying, steal, election, Biden, ...<\/td>\n",
       "      <td>25<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>1764<\/th>\n",
       "      <td>Hillary just didn't cheat enough last time. Th...<\/td>\n",
       "      <td>1<\/td>\n",
       "      <td>0.0<\/td>\n",
       "      <td>[Hillary, n't, cheat, enough, last, time, ., T...<\/td>\n",
       "      <td>33<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>1765<\/th>\n",
       "      <td>Trump has 70 million supporters... Biden has 8...<\/td>\n",
       "      <td>1<\/td>\n",
       "      <td>0.0<\/td>\n",
       "      <td>[Trump, 70, million, supporters, ..., Biden, 8...<\/td>\n",
       "      <td>22<\/td>\n",
       "    <\/tr>\n",
       "  <\/tbody>\n",
       "<\/table>\n",
       "<p>1758 rows √ó 5 columns<\/p>\n",
       "<\/div>"
      ]
     },
     "metadata":{
      
     },
     "output_type":"display_data"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"6hMLnmUggJ6kOQkH0XdKyA",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "tweet_freq_dict = nltk.FreqDist(overall_tokens)\n",
    "print(type(tweet_freq_dict))\n",
    "tweet_freq_dict.tabulate(25)"
   ],
   "execution_count":8,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "<class 'nltk.probability.FreqDist'>\n",
      "           .        Biden            ,            !           's website_name            I            ?          n't        Trump            #          Joe            :          The    President        would          ...            ;     election       people         like            &          100          amp           '' \n",
      "        2042         1660         1083          658          588          531          426          410          349          319          301          292          176          160          143          137          137          128          127          115          108          107          101          100           99 \n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"6Mq3bSy1ti9jaYAFtDvDLV",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "vocab_to_int_encoding = {pair[1]:pair[0]+1 for pair in enumerate(tweet_freq_dict)}\n",
    "print(len(vocab_to_int_encoding))\n",
    "print(type(vocab_to_int_encoding))\n",
    "vocab_to_int_encoding"
   ],
   "execution_count":9,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "7171\n",
      "<class 'dict'>\n"
     ],
     "output_type":"stream"
    },
    {
     "data":{
      "text\/plain":[
       "{'.': 1,\n",
       " 'Biden': 2,\n",
       " ',': 3,\n",
       " '!': 4,\n",
       " \"'s\": 5,\n",
       " 'website_name': 6,\n",
       " 'I': 7,\n",
       " '?': 8,\n",
       " \"n't\": 9,\n",
       " 'Trump': 10,\n",
       " '#': 11,\n",
       " 'Joe': 12,\n",
       " ':': 13,\n",
       " 'The': 14,\n",
       " 'President': 15,\n",
       " 'would': 16,\n",
       " '...': 17,\n",
       " ';': 18,\n",
       " 'election': 19,\n",
       " 'people': 20,\n",
       " 'like': 21,\n",
       " '&': 22,\n",
       " '100': 23,\n",
       " 'amp': 24,\n",
       " \"''\": 25,\n",
       " '``': 26,\n",
       " 'He': 27,\n",
       " 'days': 28,\n",
       " 'votes': 29,\n",
       " 'It': 30,\n",
       " 'know': 31,\n",
       " 'Americans': 32,\n",
       " 'You': 33,\n",
       " 'president': 34,\n",
       " 'BIDEN': 35,\n",
       " 'get': 36,\n",
       " 'going': 37,\n",
       " 'first': 38,\n",
       " '‚Äú': 39,\n",
       " '‚Äù': 40,\n",
       " \"'\": 41,\n",
       " 'wear': 42,\n",
       " 'masks': 43,\n",
       " 'says': 44,\n",
       " 'We': 45,\n",
       " 'biden': 46,\n",
       " 'administration': 47,\n",
       " 'office': 48,\n",
       " 'one': 49,\n",
       " 'fraud': 50,\n",
       " '-': 51,\n",
       " 'ask': 52,\n",
       " 'And': 53,\n",
       " 'If': 54,\n",
       " 'think': 55,\n",
       " 'via': 56,\n",
       " 'Fauci': 57,\n",
       " 'vote': 58,\n",
       " 'That': 59,\n",
       " 'could': 60,\n",
       " 'said': 61,\n",
       " ')': 62,\n",
       " 'say': 63,\n",
       " 'win': 64,\n",
       " 'still': 65,\n",
       " 'want': 66,\n",
       " \"'m\": 67,\n",
       " 'President-elect': 68,\n",
       " \"'re\": 69,\n",
       " 'got': 70,\n",
       " '(': 71,\n",
       " 'voted': 72,\n",
       " 'They': 73,\n",
       " 'US': 74,\n",
       " 'team': 75,\n",
       " 'see': 76,\n",
       " 'right': 77,\n",
       " 'What': 78,\n",
       " 'CNN': 79,\n",
       " 'This': 80,\n",
       " 'take': 81,\n",
       " 'time': 82,\n",
       " 'Harris': 83,\n",
       " 'So': 84,\n",
       " 'need': 85,\n",
       " 'way': 86,\n",
       " 'Obama': 87,\n",
       " 'But': 88,\n",
       " 'make': 89,\n",
       " 'us': 90,\n",
       " 'China': 91,\n",
       " 'even': 92,\n",
       " 'No': 93,\n",
       " '%': 94,\n",
       " 'much': 95,\n",
       " 'ballots': 96,\n",
       " 'call': 97,\n",
       " 'good': 98,\n",
       " 'How': 99,\n",
       " 'vaccine': 100,\n",
       " 'trump': 101,\n",
       " '..': 102,\n",
       " 'new': 103,\n",
       " \"'ll\": 104,\n",
       " 'years': 105,\n",
       " 'A': 106,\n",
       " 'wo': 107,\n",
       " 'mask': 108,\n",
       " 'nothing': 109,\n",
       " 'America': 110,\n",
       " \"'ve\": 111,\n",
       " 'In': 112,\n",
       " 'Just': 113,\n",
       " 'back': 114,\n",
       " 'never': 115,\n",
       " 'money': 116,\n",
       " 'evidence': 117,\n",
       " 'Georgia': 118,\n",
       " 'go': 119,\n",
       " 'already': 120,\n",
       " 'million': 121,\n",
       " 'elected': 122,\n",
       " 'Do': 123,\n",
       " 'Thank': 124,\n",
       " 'country': 125,\n",
       " 'news': 126,\n",
       " 'sure': 127,\n",
       " 'day': 128,\n",
       " '2': 129,\n",
       " 'asked': 130,\n",
       " 'hope': 131,\n",
       " 'really': 132,\n",
       " 'well': 133,\n",
       " 'lost': 134,\n",
       " 'give': 135,\n",
       " '....': 136,\n",
       " 'stay': 137,\n",
       " 'Dr.': 138,\n",
       " 'also': 139,\n",
       " 'ca': 140,\n",
       " 'To': 141,\n",
       " 'better': 142,\n",
       " 'TO': 143,\n",
       " 'There': 144,\n",
       " 'man': 145,\n",
       " 'plan': 146,\n",
       " 'American': 147,\n",
       " 'THE': 148,\n",
       " 'NOT': 149,\n",
       " 'power': 150,\n",
       " 'market': 151,\n",
       " 'Democrats': 152,\n",
       " 'Not': 153,\n",
       " 'wants': 154,\n",
       " 'next': 155,\n",
       " 'Yes': 156,\n",
       " 'PRESIDENT': 157,\n",
       " 'inauguration': 158,\n",
       " 'Because': 159,\n",
       " 'believe': 160,\n",
       " 'medical': 161,\n",
       " 'many': 162,\n",
       " 'Biden\/Harris': 163,\n",
       " 'gets': 164,\n",
       " 'voters': 165,\n",
       " 'anyone': 166,\n",
       " 'IS': 167,\n",
       " 'chief': 168,\n",
       " 'help': 169,\n",
       " 'work': 170,\n",
       " 'needs': 171,\n",
       " 'Anthony': 172,\n",
       " 'Congress': 173,\n",
       " 'thing': 174,\n",
       " 'Election': 175,\n",
       " 'put': 176,\n",
       " '‚Äò': 177,\n",
       " 'Hunter': 178,\n",
       " 'since': 179,\n",
       " 'Kamala': 180,\n",
       " 'trying': 181,\n",
       " 'keep': 182,\n",
       " 'states': 183,\n",
       " 'Republican': 184,\n",
       " 'God': 185,\n",
       " '1': 186,\n",
       " '4': 187,\n",
       " 'world': 188,\n",
       " 'Thanks': 189,\n",
       " 'support': 190,\n",
       " 'wins': 191,\n",
       " 'come': 192,\n",
       " 'state': 193,\n",
       " 'adviser': 194,\n",
       " 'elect': 195,\n",
       " 'must': 196,\n",
       " 'McConnell': 197,\n",
       " 'Biden-Harris': 198,\n",
       " 'ever': 199,\n",
       " 'Donald': 200,\n",
       " 'Republicans': 201,\n",
       " 'something': 202,\n",
       " 'start': 203,\n",
       " 'All': 204,\n",
       " 'GOP': 205,\n",
       " 'video': 206,\n",
       " 'voter': 207,\n",
       " 'actually': 208,\n",
       " 'House': 209,\n",
       " '--': 210,\n",
       " '2020': 211,\n",
       " 'Senate': 212,\n",
       " 'told': 213,\n",
       " \"'d\": 214,\n",
       " 'January': 215,\n",
       " 'stop': 216,\n",
       " 'shit': 217,\n",
       " 'change': 218,\n",
       " 'She': 219,\n",
       " 'na': 220,\n",
       " 'Elect': 221,\n",
       " 'Bernie': 222,\n",
       " 'wait': 223,\n",
       " 'mean': 224,\n",
       " 'done': 225,\n",
       " 'crash': 226,\n",
       " 'let': 227,\n",
       " 'person': 228,\n",
       " 'wearing': 229,\n",
       " 'campaign': 230,\n",
       " 'deal': 231,\n",
       " 'history': 232,\n",
       " 'year': 233,\n",
       " 'knew': 234,\n",
       " 'thinks': 235,\n",
       " 'sworn': 236,\n",
       " 'Why': 237,\n",
       " 'thought': 238,\n",
       " 'job': 239,\n",
       " '3': 240,\n",
       " 'Of': 241,\n",
       " 'Will': 242,\n",
       " 'everyone': 243,\n",
       " 'takes': 244,\n",
       " 'joe': 245,\n",
       " '$': 246,\n",
       " 'saying': 247,\n",
       " 'Well': 248,\n",
       " 'Can': 249,\n",
       " 'TRUMP': 250,\n",
       " 'bad': 251,\n",
       " 'credit': 252,\n",
       " 'anything': 253,\n",
       " 'talking': 254,\n",
       " 'reason': 255,\n",
       " 'presidency': 256,\n",
       " 'part': 257,\n",
       " 'left': 258,\n",
       " 'fake': 259,\n",
       " 'old': 260,\n",
       " 'wrong': 261,\n",
       " 'transition': 262,\n",
       " 'coronavirus': 263,\n",
       " 'find': 264,\n",
       " 'relief': 265,\n",
       " 'OF': 266,\n",
       " 'News': 267,\n",
       " 'someone': 268,\n",
       " 'Did': 269,\n",
       " 'real': 270,\n",
       " 'For': 271,\n",
       " 'Pelosi': 272,\n",
       " 'victory': 273,\n",
       " 'look': 274,\n",
       " 'last': 275,\n",
       " 'IT': 276,\n",
       " 'point': 277,\n",
       " 'leader': 278,\n",
       " 'NEVER': 279,\n",
       " 'agree': 280,\n",
       " 'made': 281,\n",
       " 'true': 282,\n",
       " 'best': 283,\n",
       " 'voting': 284,\n",
       " 'fact': 285,\n",
       " 'Or': 286,\n",
       " 'Oh': 287,\n",
       " 'Dems': 288,\n",
       " 'knows': 289,\n",
       " 'great': 290,\n",
       " 'gon': 291,\n",
       " 'tell': 292,\n",
       " 'everything': 293,\n",
       " 'things': 294,\n",
       " 'President-Elect': 295,\n",
       " 'Exclusive': 296,\n",
       " 'corrupt': 297,\n",
       " 'makes': 298,\n",
       " 'Who': 299,\n",
       " 'chance': 300,\n",
       " 'stock': 301,\n",
       " 'always': 302,\n",
       " 'cheated': 303,\n",
       " 'means': 304,\n",
       " 'COVID': 305,\n",
       " 'ass': 306,\n",
       " 'Covid': 307,\n",
       " 'Yeah': 308,\n",
       " 'future': 309,\n",
       " 'getting': 310,\n",
       " 'hear': 311,\n",
       " 'elections': 312,\n",
       " 'may': 313,\n",
       " 'When': 314,\n",
       " 'majority': 315,\n",
       " 'Let': 316,\n",
       " '80': 317,\n",
       " 'POTUS': 318,\n",
       " 'course': 319,\n",
       " 'White': 320,\n",
       " 'might': 321,\n",
       " 'Then': 322,\n",
       " 'Does': 323,\n",
       " 'YOU': 324,\n",
       " 'My': 325,\n",
       " 'soon': 326,\n",
       " 'Is': 327,\n",
       " 'helped': 328,\n",
       " 'Now': 329,\n",
       " 'use': 330,\n",
       " 'enough': 331,\n",
       " 'went': 332,\n",
       " 'After': 333,\n",
       " 'live': 334,\n",
       " 'winner': 335,\n",
       " 'place': 336,\n",
       " 'maybe': 337,\n",
       " 'away': 338,\n",
       " 'party': 339,\n",
       " 'Administration': 340,\n",
       " 'every': 341,\n",
       " 'With': 342,\n",
       " 'jobs': 343,\n",
       " 'family': 344,\n",
       " 'count': 345,\n",
       " 'yet': 346,\n",
       " 'Maybe': 347,\n",
       " 'working': 348,\n",
       " 'admin': 349,\n",
       " 'understand': 350,\n",
       " 'happened': 351,\n",
       " 'face': 352,\n",
       " 'taking': 353,\n",
       " 'Good': 354,\n",
       " 'Even': 355,\n",
       " 'Here': 356,\n",
       " 'story': 357,\n",
       " 'shows': 358,\n",
       " '7': 359,\n",
       " 'house': 360,\n",
       " 'idea': 361,\n",
       " 'JOE': 362,\n",
       " 'care': 363,\n",
       " 'tweet': 364,\n",
       " 'less': 365,\n",
       " 'government': 366,\n",
       " 'millions': 367,\n",
       " 'incoming': 368,\n",
       " 'IN': 369,\n",
       " 'electoral': 370,\n",
       " 'media': 371,\n",
       " 'LYING': 372,\n",
       " 'Democrat': 373,\n",
       " '5': 374,\n",
       " 'court': 375,\n",
       " 'AND': 376,\n",
       " '*': 377,\n",
       " 'guy': 378,\n",
       " '2021': 379,\n",
       " 'Get': 380,\n",
       " 'system': 381,\n",
       " 'democrats': 382,\n",
       " 'another': 383,\n",
       " 'Your': 384,\n",
       " 'known': 385,\n",
       " 'without': 386,\n",
       " 'latest': 387,\n",
       " 'virus': 388,\n",
       " 'lying': 389,\n",
       " 'presidential': 390,\n",
       " 'record': 391,\n",
       " 'around': 392,\n",
       " 'love': 393,\n",
       " 'acts': 394,\n",
       " 'As': 395,\n",
       " 'big': 396,\n",
       " '|': 397,\n",
       " 'USA': 398,\n",
       " 'fix': 399,\n",
       " 'months': 400,\n",
       " 'People': 401,\n",
       " 'long': 402,\n",
       " 'important': 403,\n",
       " 'thanks': 404,\n",
       " 'U.S.': 405,\n",
       " 'act': 406,\n",
       " 'times': 407,\n",
       " 'Jake': 408,\n",
       " 'Please': 409,\n",
       " 'though': 410,\n",
       " 'WILL': 411,\n",
       " 'BE': 412,\n",
       " 'Chinese': 413,\n",
       " 'term': 414,\n",
       " 'worry': 415,\n",
       " 'Hillary': 416,\n",
       " 'move': 417,\n",
       " 'From': 418,\n",
       " 'results': 419,\n",
       " 'speak': 420,\n",
       " 'beat': 421,\n",
       " 'Ca': 422,\n",
       " 'show': 423,\n",
       " 'pretty': 424,\n",
       " 'economy': 425,\n",
       " 'candidate': 426,\n",
       " 'continue': 427,\n",
       " 'talk': 428,\n",
       " 'New': 429,\n",
       " '‚Äì': 430,\n",
       " 'running': 431,\n",
       " 'action': 432,\n",
       " 'mask-wearing': 433,\n",
       " 'Clinton': 434,\n",
       " 'National': 435,\n",
       " 'night': 436,\n",
       " 'four': 437,\n",
       " 'absolutely': 438,\n",
       " 'pay': 439,\n",
       " 'First': 440,\n",
       " 'stimulus': 441,\n",
       " 'Go': 442,\n",
       " 'lot': 443,\n",
       " 'On': 444,\n",
       " 'THAT': 445,\n",
       " 'Over': 446,\n",
       " 'Mr.': 447,\n",
       " 'black': 448,\n",
       " 'hand': 449,\n",
       " 'read': 450,\n",
       " 'inaugurated': 451,\n",
       " 'Twitter': 452,\n",
       " 'recount': 453,\n",
       " 'giving': 454,\n",
       " 'fair': 455,\n",
       " 'global': 456,\n",
       " 'month': 457,\n",
       " 'Ballots': 458,\n",
       " 'Anyone': 459,\n",
       " 'today': 460,\n",
       " 'little': 461,\n",
       " 'CHEATING': 462,\n",
       " 'MY': 463,\n",
       " 'counting': 464,\n",
       " 'came': 465,\n",
       " 'failed': 466,\n",
       " 'question': 467,\n",
       " 'report': 468,\n",
       " 'Jan': 469,\n",
       " 'used': 470,\n",
       " '20': 471,\n",
       " 'Has': 472,\n",
       " 'loser': 473,\n",
       " 'NO': 474,\n",
       " 'Be': 475,\n",
       " 'urges': 476,\n",
       " 'kick': 477,\n",
       " 'üëá': 478,\n",
       " 'called': 479,\n",
       " 'Plus': 480,\n",
       " 'optimism': 481,\n",
       " 'growing': 482,\n",
       " 'Only': 483,\n",
       " 'policy': 484,\n",
       " 'Keep': 485,\n",
       " 'economic': 486,\n",
       " 'seen': 487,\n",
       " 'guess': 488,\n",
       " 'feel': 489,\n",
       " 'dog': 490,\n",
       " 'lies': 491,\n",
       " 'wonder': 492,\n",
       " 'coup': 493,\n",
       " '.....': 494,\n",
       " 'COVID-19': 495,\n",
       " 'truth': 496,\n",
       " 'bet': 497,\n",
       " 'One': 498,\n",
       " 'making': 499,\n",
       " 'trust': 500,\n",
       " 'forward': 501,\n",
       " 'war': 502,\n",
       " 'lose': 503,\n",
       " 'rid': 504,\n",
       " 'sign': 505,\n",
       " 'control': 506,\n",
       " 'explain': 507,\n",
       " 'tells': 508,\n",
       " 'lives': 509,\n",
       " 'join': 510,\n",
       " 'hard': 511,\n",
       " 'sense': 512,\n",
       " 'matter': 513,\n",
       " 'ballot': 514,\n",
       " 'prison': 515,\n",
       " 'ahead': 516,\n",
       " 'General': 517,\n",
       " 'Pentagon': 518,\n",
       " 'military': 519,\n",
       " 'cabinet': 520,\n",
       " 'least': 521,\n",
       " 'supporters': 522,\n",
       " 'Court': 523,\n",
       " 'His': 524,\n",
       " 'biggest': 525,\n",
       " 'fire': 526,\n",
       " 'case': 527,\n",
       " 'conspiracy': 528,\n",
       " 'Some': 529,\n",
       " 'markets': 530,\n",
       " 'response': 531,\n",
       " 'home': 532,\n",
       " 'Little': 533,\n",
       " 'Thatcher': 534,\n",
       " '1982': 535,\n",
       " 'Argie': 536,\n",
       " 'Lexington': 537,\n",
       " 'Raid': 538,\n",
       " '1831': 539,\n",
       " 'p.101': 540,\n",
       " 'moron': 541,\n",
       " 'watch': 542,\n",
       " 'Like': 543,\n",
       " 'Dominion': 544,\n",
       " 'bill': 545,\n",
       " 'race': 546,\n",
       " 'single': 547,\n",
       " 'criminal': 548,\n",
       " 'asking': 549,\n",
       " 'woman': 550,\n",
       " 'lol': 551,\n",
       " 'Barr': 552,\n",
       " 'fuck': 553,\n",
       " 'wanted': 554,\n",
       " 'name': 555,\n",
       " 'Kellyanne': 556,\n",
       " 'workers': 557,\n",
       " 'Nothing': 558,\n",
       " 'likely': 559,\n",
       " 'WON': 560,\n",
       " 'Says': 561,\n",
       " 'u': 562,\n",
       " 'position': 563,\n",
       " 'intelligence': 564,\n",
       " 'respect': 565,\n",
       " 'loss': 566,\n",
       " 'Remember': 567,\n",
       " 'FRAUD': 568,\n",
       " 'accept': 569,\n",
       " 'tweets': 570,\n",
       " 'bc': 571,\n",
       " 'Tapper': 572,\n",
       " 'probably': 573,\n",
       " 'Are': 574,\n",
       " 'destroy': 575,\n",
       " 'clear': 576,\n",
       " 'MSM': 577,\n",
       " 'IF': 578,\n",
       " 'else': 579,\n",
       " 'ready': 580,\n",
       " 'idiot': 581,\n",
       " 'heard': 582,\n",
       " 'table': 583,\n",
       " 'DO': 584,\n",
       " 'BREAKING': 585,\n",
       " '‚Äî': 586,\n",
       " 'cases': 587,\n",
       " 'political': 588,\n",
       " 'Trumps': 589,\n",
       " 'legal': 590,\n",
       " '10': 591,\n",
       " 'far': 592,\n",
       " 'WH': 593,\n",
       " 'caught': 594,\n",
       " 'Supreme': 595,\n",
       " 'half': 596,\n",
       " 'members': 597,\n",
       " 'instead': 598,\n",
       " 'ticket': 599,\n",
       " 'Votes': 600,\n",
       " 'choice': 601,\n",
       " 'eyes': 602,\n",
       " 'nation': 603,\n",
       " 'small': 604,\n",
       " 'order': 605,\n",
       " 'Pres': 606,\n",
       " 'especially': 607,\n",
       " 'cares': 608,\n",
       " '20th': 609,\n",
       " 'coming': 610,\n",
       " 'high': 611,\n",
       " 'FBI': 612,\n",
       " 'Fraud': 613,\n",
       " 'aid': 614,\n",
       " 'nationwide': 615,\n",
       " 'recovery': 616,\n",
       " 'dropped': 617,\n",
       " 'hell': 618,\n",
       " 'proof': 619,\n",
       " 'expected': 620,\n",
       " 'stole': 621,\n",
       " 'Inauguration': 622,\n",
       " '47': 623,\n",
       " 'cheating': 624,\n",
       " 'stupid': 625,\n",
       " 'thats': 626,\n",
       " 'FoxNews': 627,\n",
       " 'proven': 628,\n",
       " 'GA': 629,\n",
       " 'guys': 630,\n",
       " 'sorry': 631,\n",
       " 'damn': 632,\n",
       " '6': 633,\n",
       " 'Have': 634,\n",
       " 'loves': 635,\n",
       " 'official': 636,\n",
       " 'Conway': 637,\n",
       " 'agenda': 638,\n",
       " 'claims': 639,\n",
       " 'NOTHING': 640,\n",
       " 'took': 641,\n",
       " 'become': 642,\n",
       " 'Iran': 643,\n",
       " 'trade': 644,\n",
       " 'behind': 645,\n",
       " 'More': 646,\n",
       " 'democracy': 647,\n",
       " 'U': 648,\n",
       " 'forget': 649,\n",
       " 'listen': 650,\n",
       " 'Way': 651,\n",
       " 'leadership': 652,\n",
       " 'Day': 653,\n",
       " 'due': 654,\n",
       " 'audit': 655,\n",
       " 'Black': 656,\n",
       " 'spy': 657,\n",
       " 'machine': 658,\n",
       " 'play': 659,\n",
       " 'Also': 660,\n",
       " 'crap': 661,\n",
       " 'massive': 662,\n",
       " 'counted': 663,\n",
       " '3rd': 664,\n",
       " 'NY': 665,\n",
       " 'popular': 666,\n",
       " 'different': 667,\n",
       " 'numbers': 668,\n",
       " 'fighting': 669,\n",
       " 'problem': 670,\n",
       " 'two': 671,\n",
       " 'Win': 672,\n",
       " 'looks': 673,\n",
       " 'plans': 674,\n",
       " 'Stop': 675,\n",
       " 'seem': 676,\n",
       " 'goes': 677,\n",
       " 'die': 678,\n",
       " 'dems': 679,\n",
       " 'entire': 680,\n",
       " 'Electoral': 681,\n",
       " 'agencies': 682,\n",
       " 'Mitch': 683,\n",
       " 'picks': 684,\n",
       " 'States': 685,\n",
       " 'losing': 686,\n",
       " 'resign': 687,\n",
       " 'gone': 688,\n",
       " 'almost': 689,\n",
       " 'kind': 690,\n",
       " 'corruption': 691,\n",
       " 'using': 692,\n",
       " 'fired': 693,\n",
       " 'issue': 694,\n",
       " 'pass': 695,\n",
       " 'likes': 696,\n",
       " 'current': 697,\n",
       " 'Thursday': 698,\n",
       " 'past': 699,\n",
       " 'GET': 700,\n",
       " 'glad': 701,\n",
       " 'reality': 702,\n",
       " 'sad': 703,\n",
       " 'Barack': 704,\n",
       " 'sick': 705,\n",
       " 'planning': 706,\n",
       " 'cast': 707,\n",
       " 'tax': 708,\n",
       " 'argument': 709,\n",
       " 'account': 710,\n",
       " 'liar': 711,\n",
       " 'end': 712,\n",
       " 'leaders': 713,\n",
       " 'Would': 714,\n",
       " 'puppet': 715,\n",
       " 'word': 716,\n",
       " 'Among': 717,\n",
       " 'mail': 718,\n",
       " 'Still': 719,\n",
       " 'integrity': 720,\n",
       " 'white': 721,\n",
       " 'law': 722,\n",
       " 'Thats': 723,\n",
       " 'run': 724,\n",
       " 'comes': 725,\n",
       " 'post': 726,\n",
       " 'revealed': 727,\n",
       " 'TV': 728,\n",
       " 'sounds': 729,\n",
       " 'acknowledges': 730,\n",
       " 'apparent': 731,\n",
       " 'BS': 732,\n",
       " 'weeks': 733,\n",
       " 'hold': 734,\n",
       " 'affidavits': 735,\n",
       " 'death': 736,\n",
       " 'Democratic': 737,\n",
       " 'return': 738,\n",
       " 'disease': 739,\n",
       " 'communist': 740,\n",
       " 'Dr': 741,\n",
       " 'either': 742,\n",
       " 'KNOW': 743,\n",
       " 'public': 744,\n",
       " 'game': 745,\n",
       " 'FOR': 746,\n",
       " 'college': 747,\n",
       " 'basement': 748,\n",
       " 'zero': 749,\n",
       " 'together': 750,\n",
       " 'doubt': 751,\n",
       " 'Covid-19': 752,\n",
       " 'republicans': 753,\n",
       " 'EVIDENCE': 754,\n",
       " 'CAMPAIGN': 755,\n",
       " 'showing': 756,\n",
       " 'investigation': 757,\n",
       " 'happen': 758,\n",
       " 'Dem': 759,\n",
       " 'VP': 760,\n",
       " 'fight': 761,\n",
       " 'telling': 762,\n",
       " 'ALL': 763,\n",
       " 'COVID19': 764,\n",
       " 'longer': 765,\n",
       " 'pushing': 766,\n",
       " 'certified': 767,\n",
       " 'try': 768,\n",
       " 'thank': 769,\n",
       " 'MORE': 770,\n",
       " 'blocks': 771,\n",
       " 'visits': 772,\n",
       " 'AZ': 773,\n",
       " 'landslide': 774,\n",
       " 'officials': 775,\n",
       " '2024': 776,\n",
       " 'winning': 777,\n",
       " 'foot': 778,\n",
       " 'full': 779,\n",
       " 'lead': 780,\n",
       " 'Mr': 781,\n",
       " 'following': 782,\n",
       " 'wake': 783,\n",
       " 'realize': 784,\n",
       " 'homes': 785,\n",
       " 'hurt': 786,\n",
       " 'lack': 787,\n",
       " 'luck': 788,\n",
       " 'steal': 789,\n",
       " 'safe': 790,\n",
       " 'Perdue': 791,\n",
       " '8': 792,\n",
       " 'College': 793,\n",
       " 'dead': 794,\n",
       " 'These': 795,\n",
       " 'calls': 796,\n",
       " 'poll': 797,\n",
       " 'police': 798,\n",
       " 'found': 799,\n",
       " 'Every': 800,\n",
       " 'rigged': 801,\n",
       " 'stuff': 802,\n",
       " 'appointed': 803,\n",
       " 'joke': 804,\n",
       " 'whole': 805,\n",
       " 'civil': 806,\n",
       " 'Pennsylvania': 807,\n",
       " 'men': 808,\n",
       " 'correct': 809,\n",
       " 'whether': 810,\n",
       " 'MAGA': 811,\n",
       " 'head': 812,\n",
       " 'illegal': 813,\n",
       " 'line': 814,\n",
       " 'board': 815,\n",
       " 'talks': 816,\n",
       " 'WE': 817,\n",
       " '\/': 818,\n",
       " 'DNC': 819,\n",
       " 'candidates': 820,\n",
       " 'key': 821,\n",
       " 'changed': 822,\n",
       " 'Call': 823,\n",
       " 'set': 824,\n",
       " 'LOSER': 825,\n",
       " 'mandatory': 826,\n",
       " 'Except': 827,\n",
       " 'Nobody': 828,\n",
       " 'age': 829,\n",
       " 'Give': 830,\n",
       " 'looking': 831,\n",
       " 'foreign': 832,\n",
       " 'challenges': 833,\n",
       " 'amid': 834,\n",
       " 'Climate': 835,\n",
       " 'lied': 836,\n",
       " 'Same': 837,\n",
       " 'dementia': 838,\n",
       " 'HE': 839,\n",
       " 'including': 840,\n",
       " 'Fox': 841,\n",
       " 'watching': 842,\n",
       " 'number': 843,\n",
       " 'waiting': 844,\n",
       " 'Big': 845,\n",
       " 'Transition': 846,\n",
       " 'clown': 847,\n",
       " 'ü§°': 848,\n",
       " 'However': 849,\n",
       " 'easily': 850,\n",
       " 'president-elect': 851,\n",
       " 'facts': 852,\n",
       " 'Where': 853,\n",
       " 'worse': 854,\n",
       " 'calling': 855,\n",
       " 'AG': 856,\n",
       " 'Watch': 857,\n",
       " 'n': 858,\n",
       " 'second': 859,\n",
       " 'chaos': 860,\n",
       " 'reported': 861,\n",
       " 'Everyone': 862,\n",
       " '45': 863,\n",
       " 'F': 864,\n",
       " 'huge': 865,\n",
       " 'Christmas': 866,\n",
       " 'front': 867,\n",
       " 'possible': 868,\n",
       " 'Nancy': 869,\n",
       " 'close': 870,\n",
       " 'üá∫üá∏': 871,\n",
       " 'afraid': 872,\n",
       " 'Say': 873,\n",
       " 'deserves': 874,\n",
       " 'per': 875,\n",
       " 'status': 876,\n",
       " 'article': 877,\n",
       " 'Were': 878,\n",
       " 'ABOUT': 879,\n",
       " 'wars': 880,\n",
       " 'MILLION': 881,\n",
       " 'VOTE': 882,\n",
       " 'WTF': 883,\n",
       " 'brought': 884,\n",
       " 'Tanden': 885,\n",
       " 'difference': 886,\n",
       " 'hit': 887,\n",
       " 'ago': 888,\n",
       " 'Party': 889,\n",
       " 'check': 890,\n",
       " 'ends': 891,\n",
       " \"N'T\": 892,\n",
       " 'Russia': 893,\n",
       " 'policies': 894,\n",
       " 'Business': 895,\n",
       " 'Wow': 896,\n",
       " 'crime': 897,\n",
       " 'free': 898,\n",
       " 'impossible': 899,\n",
       " 'cover': 900,\n",
       " 'politics': 901,\n",
       " 'social': 902,\n",
       " 'swamp': 903,\n",
       " 'followers': 904,\n",
       " 'liars': 905,\n",
       " 'THIS': 906,\n",
       " 'deals': 907,\n",
       " 'hopefully': 908,\n",
       " 'acknowledge': 909,\n",
       " 'Wisconsin': 910,\n",
       " 'camp': 911,\n",
       " 'somehow': 912,\n",
       " 'funny': 913,\n",
       " 'remember': 914,\n",
       " 'health': 915,\n",
       " 'challenge': 916,\n",
       " 'cult': 917,\n",
       " 'business': 918,\n",
       " \"'S\": 919,\n",
       " 'child': 920,\n",
       " 'anyway': 921,\n",
       " 'national': 922,\n",
       " 'Great': 923,\n",
       " 'Better': 924,\n",
       " 'commit': 925,\n",
       " 'Financial': 926,\n",
       " 'swing': 927,\n",
       " 'involved': 928,\n",
       " 'worn': 929,\n",
       " 'staying': 930,\n",
       " 'confidence': 931,\n",
       " 'leaving': 932,\n",
       " 'becomes': 933,\n",
       " 'answer': 934,\n",
       " 'Wall': 935,\n",
       " 'Street': 936,\n",
       " 'evil': 937,\n",
       " 'Michigan': 938,\n",
       " 'While': 939,\n",
       " 'rally': 940,\n",
       " 'reduction': 941,\n",
       " 'drive': 942,\n",
       " 'OCC': 943,\n",
       " 'covid': 944,\n",
       " 'shut': 945,\n",
       " 'BUT': 946,\n",
       " 'fault': 947,\n",
       " 'anymore': 948,\n",
       " 'narrative': 949,\n",
       " 'remove': 950,\n",
       " 'Ever': 951,\n",
       " 'Schumer': 952,\n",
       " 'Really': 953,\n",
       " 'nominee': 954,\n",
       " 'Lol': 955,\n",
       " 'Im': 956,\n",
       " 'words': 957,\n",
       " 'pandemic': 958,\n",
       " 'United': 959,\n",
       " 'OVER': 960,\n",
       " 'fool': 961,\n",
       " 'Exactly': 962,\n",
       " 'exactly': 963,\n",
       " 'DOJ': 964,\n",
       " 'climate': 965,\n",
       " 'wish': 966,\n",
       " 'fraudulent': 967,\n",
       " 'yes': 968,\n",
       " 'dont': 969,\n",
       " 'focus': 970,\n",
       " 'boring': 971,\n",
       " 'Hey': 972,\n",
       " 'upon': 973,\n",
       " 'Arizona': 974,\n",
       " 'stopped': 975,\n",
       " 'federal': 976,\n",
       " 'At': 977,\n",
       " '2016': 978,\n",
       " 'ARE': 979,\n",
       " 'week': 980,\n",
       " 'boxes': 981,\n",
       " 'pardon': 982,\n",
       " 'Team': 983,\n",
       " 'energy': 984,\n",
       " 'pull': 985,\n",
       " 'continues': 986,\n",
       " 'straight': 987,\n",
       " 'obvious': 988,\n",
       " 'square': 989,\n",
       " 'NOW': 990,\n",
       " 'open': 991,\n",
       " 'laptop': 992,\n",
       " 'dollars': 993,\n",
       " 'bring': 994,\n",
       " 'leave': 995,\n",
       " 'Texas': 996,\n",
       " 'ai': 997,\n",
       " 'gt': 998,\n",
       " 'Truth': 999,\n",
       " 'raise': 1000,\n",
       " ...}"
      ]
     },
     "metadata":{
      
     },
     "output_type":"display_data"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"m5uJcYi3OTa5cpuqAQTAF0",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "truncate_to_unknown_corpus_length_limit = 5000\n",
    "assert truncate_to_unknown_corpus_length_limit < len(vocab_to_int_encoding), \"unknown truncation limit must be smaller than corpus length\"\n",
    "vocab_to_int_encoding[\"<unk>\"] = truncate_to_unknown_corpus_length_limit + 1\n",
    "\n",
    "def tokens_to_int(x: pd.Series):\n",
    "    tokens = x[token_col_name]\n",
    "    try:\n",
    "        tokens_in_int = [vocab_to_int_encoding[token] for token in tokens]\n",
    "        for idx in range(len(tokens_in_int)):\n",
    "            if tokens_in_int[idx] >= truncate_to_unknown_corpus_length_limit:\n",
    "                tokens_in_int[idx] = truncate_to_unknown_corpus_length_limit+1\n",
    "    except KeyError:\n",
    "        print(x)\n",
    "        return -1\n",
    "    return tokens_in_int\n",
    "\n",
    "tweets_csv[tokenized_col_name] = tweets_csv.apply(tokens_to_int, axis=1)\n"
   ],
   "execution_count":10,
   "outputs":[
    {
     "name":"stderr",
     "text":[
      "<ipython-input-10-575e6bddf1b8>:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https:\/\/pandas.pydata.org\/pandas-docs\/stable\/user_guide\/indexing.html#returning-a-view-versus-a-copy\n",
      "  tweets_csv[tokenized_col_name] = tweets_csv.apply(tokens_to_int, axis=1)\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"NBo8UjC1HT0zBeL1d7VVKK",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "bigram_finder = nltk.collocations.BigramCollocationFinder.from_words(overall_tokens)\n",
    "bigram_finder.ngram_fd.tabulate(10)"
   ],
   "execution_count":11,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "     ('Joe', 'Biden')            ('!', '!')       ('Biden', \"'s\")        ('.', 'Biden')        ('Biden', '.') ('.', 'website_name')            ('.', 'I')          ('&', 'amp')          ('amp', ';')       ('100', 'days') \n",
      "                  259                   184                   146                   126                   112                   107                   106                   100                   100                    84 \n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"YRAD9gw2Z7APjHYN2BlyiX",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "trigram_finder = nltk.collocations.TrigramCollocationFinder.from_words(overall_tokens)\n",
    "trigram_finder.ngram_fd.tabulate(10)"
   ],
   "execution_count":12,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "                  ('&', 'amp', ';')                     ('!', '!', '!')      ('Americans', 'wear', 'masks')        ('ask', 'Americans', 'wear')            ('first', '100', 'days')           ('masks', 'first', '100')          ('wear', 'masks', 'first') ('President-elect', 'Joe', 'Biden')                   ('.', 'It', \"'s\")              ('Joe', 'Biden', \"'s\") \n",
      "                                100                                  83                                  46                                  44                                  39                                  34                                  33                                  32                                  29                                  26 \n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"oL2ULJ627W8fSfo1qSsVCE",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "quadgram_finder = nltk.collocations.QuadgramCollocationFinder.from_words(overall_tokens)\n",
    "quadgram_finder.ngram_fd.tabulate(10)"
   ],
   "execution_count":13,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "   ('ask', 'Americans', 'wear', 'masks')        ('masks', 'first', '100', 'days')                     ('!', '!', '!', '!')  ('Americans', 'wear', 'masks', 'first')        ('wear', 'masks', 'first', '100')    ('Biden', 'says', 'ask', 'Americans')     ('says', 'ask', 'Americans', 'wear')    ('Biden', 'ask', 'Americans', 'wear') ('Fauci', 'chief', 'medical', 'adviser')      ('Exclusive', ':', 'Biden', 'says') \n",
      "                                      41                                       34                                       34                                       33                                       33                                       20                                       19                                       18                                       18                                       15 \n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"ijRghNk1Y4mST2vyKk4XcZ",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "\n",
    "def tweet_find_nltk_polarity(x: pd.Series):\n",
    "    senti = sia.polarity_scores(x[text_col_name])\n",
    "    return senti['compound']\n",
    "\n",
    "\n",
    "tweets_csv[ref_sentiment_name] = tweets_csv.apply(tweet_find_nltk_polarity, axis=1)\n"
   ],
   "execution_count":14,
   "outputs":[
    {
     "name":"stderr",
     "text":[
      "<ipython-input-14-f930521e6e81>:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https:\/\/pandas.pydata.org\/pandas-docs\/stable\/user_guide\/indexing.html#returning-a-view-versus-a-copy\n",
      "  tweets_csv[ref_sentiment_name] = tweets_csv.apply(tweet_find_nltk_polarity, axis=1)\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"uCNH4xe2QVlpemnkDNj49v",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "# pad features\n",
    "def pad_tokens(x: pd.Series):\n",
    "    tokens = x[tokenized_col_name]\n",
    "    padding = [0] * (50 - len(tokens))\n",
    "    return padding + tokens\n",
    "\n",
    "tweets_csv.loc[:, tokenized_col_name] = tweets_csv.apply(pad_tokens, axis=1)\n",
    "tweets_csv"
   ],
   "execution_count":15,
   "outputs":[
    {
     "name":"stderr",
     "text":[
      "\/opt\/python\/envs\/default\/lib\/python3.8\/site-packages\/pandas\/core\/indexing.py:1773: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https:\/\/pandas.pydata.org\/pandas-docs\/stable\/user_guide\/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(ilocs[0], value, pi)\n"
     ],
     "output_type":"stream"
    },
    {
     "data":{
      "text\/html":[
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "<\/style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th><\/th>\n",
       "      <th>Text<\/th>\n",
       "      <th>subjectivity<\/th>\n",
       "      <th>polarity<\/th>\n",
       "      <th>Raw tokens<\/th>\n",
       "      <th>Token length<\/th>\n",
       "      <th>Tokenized<\/th>\n",
       "      <th>NLTK ref sentiment<\/th>\n",
       "    <\/tr>\n",
       "  <\/thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0<\/th>\n",
       "      <td>That's the guy who is funding those fake stori...<\/td>\n",
       "      <td>1<\/td>\n",
       "      <td>1.0<\/td>\n",
       "      <td>[That, 's, guy, funding, fake, stories, Hunter...<\/td>\n",
       "      <td>9<\/td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...<\/td>\n",
       "      <td>-0.4767<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>1<\/th>\n",
       "      <td>Biden apparently just told JTaps that he's goi...<\/td>\n",
       "      <td>1<\/td>\n",
       "      <td>0.0<\/td>\n",
       "      <td>[Biden, apparently, told, JTaps, 's, going, as...<\/td>\n",
       "      <td>22<\/td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...<\/td>\n",
       "      <td>0.2732<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>2<\/th>\n",
       "      <td>They've been given 40 chances. And have blown ...<\/td>\n",
       "      <td>1<\/td>\n",
       "      <td>0.0<\/td>\n",
       "      <td>[They, 've, given, 40, chances, ., And, blown,...<\/td>\n",
       "      <td>40<\/td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 73, 111, 1743, ...<\/td>\n",
       "      <td>0.8442<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>3<\/th>\n",
       "      <td>They could not raise the money to beat Biden b...<\/td>\n",
       "      <td>1<\/td>\n",
       "      <td>1.0<\/td>\n",
       "      <td>[They, could, raise, money, beat, Biden, elect...<\/td>\n",
       "      <td>23<\/td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...<\/td>\n",
       "      <td>0.5367<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>4<\/th>\n",
       "      <td>Can't Biden just fire the board members on the...<\/td>\n",
       "      <td>1<\/td>\n",
       "      <td>0.0<\/td>\n",
       "      <td>[Ca, n't, Biden, fire, board, members, postal,...<\/td>\n",
       "      <td>11<\/td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...<\/td>\n",
       "      <td>0.2584<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>...<\/th>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "      <td>...<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>1761<\/th>\n",
       "      <td>That's nice, but I hope Biden doesn't think #M...<\/td>\n",
       "      <td>0<\/td>\n",
       "      <td>0.5<\/td>\n",
       "      <td>[That, 's, nice, ,, I, hope, Biden, n't, think...<\/td>\n",
       "      <td>12<\/td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...<\/td>\n",
       "      <td>0.6956<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>1762<\/th>\n",
       "      <td>OMG. You are a sensitive soul. For the record ...<\/td>\n",
       "      <td>1<\/td>\n",
       "      <td>1.0<\/td>\n",
       "      <td>[OMG, ., You, sensitive, soul, ., For, record,...<\/td>\n",
       "      <td>38<\/td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1259, 1, ...<\/td>\n",
       "      <td>0.9001<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>1763<\/th>\n",
       "      <td>No, IQ45 is trying to steal the election from ...<\/td>\n",
       "      <td>1<\/td>\n",
       "      <td>1.0<\/td>\n",
       "      <td>[No, ,, IQ45, trying, steal, election, Biden, ...<\/td>\n",
       "      <td>25<\/td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...<\/td>\n",
       "      <td>-0.8074<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>1764<\/th>\n",
       "      <td>Hillary just didn't cheat enough last time. Th...<\/td>\n",
       "      <td>1<\/td>\n",
       "      <td>0.0<\/td>\n",
       "      <td>[Hillary, n't, cheat, enough, last, time, ., T...<\/td>\n",
       "      <td>33<\/td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...<\/td>\n",
       "      <td>-0.5659<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>1765<\/th>\n",
       "      <td>Trump has 70 million supporters... Biden has 8...<\/td>\n",
       "      <td>1<\/td>\n",
       "      <td>0.0<\/td>\n",
       "      <td>[Trump, 70, million, supporters, ..., Biden, 8...<\/td>\n",
       "      <td>22<\/td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...<\/td>\n",
       "      <td>-0.8885<\/td>\n",
       "    <\/tr>\n",
       "  <\/tbody>\n",
       "<\/table>\n",
       "<p>1758 rows √ó 7 columns<\/p>\n",
       "<\/div>"
      ]
     },
     "metadata":{
      
     },
     "output_type":"display_data"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"xmMHBEmqwnty4hZYgriQ25",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "# hyperparams for learning\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "lr=0.000125\n",
    "\n",
    "train_on_gpu = False"
   ],
   "execution_count":16,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"dT75f7piUVkkNhYmVjrK03",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "tokens_full_series = tweets_csv[tokenized_col_name]\n",
    "polarity_full_series = tweets_csv[polarity_label_name]\n",
    "# tokens_full_series.to_list()"
   ],
   "execution_count":17,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"fYQQ6Q5pC3pvJuEFuWH8RF",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "\n",
    "tokens_full_nparr = np.asarray(tokens_full_series.to_list(), dtype=int) \n",
    "np.random.shuffle(tokens_full_nparr)\n",
    "\n",
    "train_valid_split_point = int(len(tokens_full_nparr)*0.8)\n",
    "valid_test_split_point = int(len(tokens_full_nparr)*0.9)\n",
    "\n",
    "train_tokens = tokens_full_nparr[ : train_valid_split_point]\n",
    "valid_tokens = tokens_full_nparr[train_valid_split_point : valid_test_split_point]\n",
    "test_tokens = tokens_full_nparr[valid_test_split_point : ]\n",
    "\n",
    "polarity_full_nparr = np.asarray(polarity_full_series.to_list(), dtype=int) \n",
    "np.random.shuffle(polarity_full_nparr)\n",
    "\n",
    "train_polarity = polarity_full_nparr[ : train_valid_split_point]\n",
    "valid_polarity = polarity_full_nparr[train_valid_split_point : valid_test_split_point]\n",
    "test_polarity = polarity_full_nparr[valid_test_split_point : ]\n",
    "\n",
    "\n",
    "\n",
    "full_data = TensorDataset(torch.from_numpy(tokens_full_nparr), torch.from_numpy(polarity_full_nparr))\n",
    "train_data = TensorDataset(torch.from_numpy(train_tokens), torch.from_numpy(train_polarity))\n",
    "valid_data = TensorDataset(torch.from_numpy(valid_tokens), torch.from_numpy(valid_tokens))\n",
    "test_data = TensorDataset(torch.from_numpy(test_tokens), torch.from_numpy(test_polarity))\n",
    "\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"
   ],
   "execution_count":18,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"tLfGVPIbGmGI0MW5JWseIc",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "class SentimentLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    The RNN model that will be used to perform Sentiment analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the model by setting up the layers.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # embedding and LSTM layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        if n_layers == 1:\n",
    "            drop_prob = 0\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, \n",
    "                            hidden_size=hidden_dim, \n",
    "                            num_layers=n_layers, \n",
    "                            dropout=drop_prob, \n",
    "                            batch_first=True)\n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        # linear and sigmoid layers\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "    \"\"\"\n",
    "    Expected hidden[0] size (2, 14, 256), got [2, 16, 256]\n",
    "    lstm_stack_size  batch_size  hidden_dim_size\n",
    "    \"\"\"\n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on some input and hidden state.\n",
    "        \"\"\"\n",
    "        # print(f\">>> in forward, size of x: {x.size()}, size of hidden: {len(hidden)}, \"\n",
    "        #       f\"size of hidden[0]: {hidden[0].size()}\")\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # embeddings and lstm_out\n",
    "        embeds = self.embedding(x)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "    \n",
    "        # stack up lstm outputs\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        # dropout and fully-connected layer\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "        # sigmoid function\n",
    "        sig_out = self.sig(out)\n",
    "        \n",
    "        # reshape to be batch_size first\n",
    "        sig_out = sig_out.view(batch_size, -1)\n",
    "        sig_out = sig_out[:, -1] # get last batch of labels\n",
    "        \n",
    "        # return last sigmoid output and hidden state\n",
    "        return sig_out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\" Initializes hidden state \"\"\"\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if train_on_gpu:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        \n",
    "        return hidden"
   ],
   "execution_count":19,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"B47qkGBo9yn7jut9ks7Xqd",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "\n",
    "vocab_size = truncate_to_unknown_corpus_length_limit+2 # +1 for the 0 padding, +1 for unknown words\n",
    "output_size = 1\n",
    "embedding_dim = 100\n",
    "hidden_dim = 128\n",
    "n_layers = 1\n",
    "net = SentimentLSTM(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
    "net"
   ],
   "execution_count":20,
   "outputs":[
    {
     "data":{
      "text\/plain":[
       "SentimentLSTM(\n",
       "  (embedding): Embedding(5002, 100)\n",
       "  (lstm): LSTM(100, 128, batch_first=True)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (sig): Sigmoid()\n",
       ")"
      ]
     },
     "metadata":{
      
     },
     "output_type":"display_data"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"7fe2aFWouohmdpqT97oVnI",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "# loss and optimization functions\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "# training params\n",
    "\n",
    "epochs = 6\n",
    "\n",
    "counter = 0\n",
    "print_every = 100\n",
    "clip=5 # gradient clipping\n",
    "\n",
    "# move model to GPU, if available\n",
    "if train_on_gpu:\n",
    "    net.cuda()\n",
    "\n",
    "net.train()\n",
    "# train for some number of epochs\n",
    "for e in range(epochs):\n",
    "    # initialize hidden state\n",
    "    h = net.init_hidden(batch_size)\n",
    "\n",
    "    # batch loop\n",
    "    for inputs, labels in train_loader:\n",
    "        counter += 1\n",
    "        if len(inputs) < batch_size:\n",
    "            print(f\"last iter of training samples are not consistent with batch size, skip though\")\n",
    "        else:\n",
    "        \n",
    "\n",
    "            if train_on_gpu:\n",
    "                inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            h = tuple([each.data for each in h])\n",
    "\n",
    "            # zero accumulated gradients\n",
    "            net.zero_grad()\n",
    "\n",
    "            # get the output from the model\n",
    "            inputs = inputs.type(torch.LongTensor)\n",
    "            output, h = net(inputs, h)\n",
    "\n",
    "            # calculate the loss and perform backprop\n",
    "            loss = criterion(output.squeeze(), labels.float())\n",
    "            loss.backward()\n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs \/ LSTMs.\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            optimizer.step()\n",
    "\n",
    "        # loss stats\n",
    "        if counter % print_every == 0:\n",
    "            # Get validation loss\n",
    "            val_h = net.init_hidden(batch_size)\n",
    "            val_losses = []\n",
    "            net.eval()\n",
    "            for inputs_in, labels_in in valid_loader:\n",
    "\n",
    "                # Creating new variables for the hidden state, otherwise\n",
    "                # we'd backprop through the entire training history\n",
    "                val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "                if train_on_gpu:\n",
    "                    inputs_in, labels_in = inputs_in.cuda(), labels_in.cuda()\n",
    "\n",
    "                inputs_in = inputs_in.type(torch.LongTensor)\n",
    "                output, val_h = net(inputs_in, val_h)\n",
    "                val_loss = criterion(output.squeeze(), labels.float())\n",
    "\n",
    "                val_losses.append(val_loss.item())\n",
    "\n",
    "            net.train()\n",
    "            print(\"Epoch: {}\/{}...\".format(e+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))"
   ],
   "execution_count":21,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "last iter of training samples are not consistent with batch size, skip though\n",
      "Epoch: 2\/6... Step: 100... Loss: 0.622651... Val Loss: 0.675567\n",
      "last iter of training samples are not consistent with batch size, skip though\n",
      "Epoch: 3\/6... Step: 200... Loss: 0.650788... Val Loss: 0.629772\n",
      "last iter of training samples are not consistent with batch size, skip though\n",
      "Epoch: 4\/6... Step: 300... Loss: 0.501737... Val Loss: 0.522649\n",
      "last iter of training samples are not consistent with batch size, skip though\n",
      "Epoch: 5\/6... Step: 400... Loss: 0.526326... Val Loss: 0.569132\n",
      "last iter of training samples are not consistent with batch size, skip though\n",
      "Epoch: 6\/6... Step: 500... Loss: 0.590776... Val Loss: 0.616621\n",
      "last iter of training samples are not consistent with batch size, skip though\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"BoX0wtUrvpRklyjvVDRtqq",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "# Get test data loss and accuracy\n",
    "\n",
    "test_losses = [] # track loss\n",
    "num_correct = 0\n",
    "\n",
    "# init hidden state\n",
    "h = net.init_hidden(batch_size)\n",
    "\n",
    "net.eval()\n",
    "# iterate over test data\n",
    "for inputs, labels in test_loader:\n",
    "\n",
    "    # Creating new variables for the hidden state, otherwise\n",
    "    # we'd backprop through the entire training history\n",
    "    h = tuple([each.data for each in h])\n",
    "\n",
    "    if train_on_gpu:\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "    \n",
    "    # get predicted outputs\n",
    "    inputs = inputs.type(torch.LongTensor)\n",
    "    output, h = net(inputs, h)\n",
    "    \n",
    "    # calculate loss\n",
    "    test_loss = criterion(output.squeeze(), labels.float())\n",
    "    test_losses.append(test_loss.item())\n",
    "    \n",
    "    # convert output probabilities to predicted class (0 or 1)\n",
    "    pred = torch.round(output.squeeze())  # rounds to the nearest integer\n",
    "    \n",
    "    # compare predictions to true label\n",
    "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "\n",
    "\n",
    "# avg test loss\n",
    "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
    "\n",
    "# accuracy over all test data\n",
    "test_acc = num_correct\/len(test_loader.dataset)\n",
    "print(\"Test accuracy: {:.3f}\".format(test_acc))"
   ],
   "execution_count":22,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "Test loss: 0.608\n",
      "Test accuracy: 0.705\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"SSTRqIWHtGGLm75ahAQalK",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "print(batch_size)\n",
    "print(lr)\n",
    "print(epochs)"
   ],
   "execution_count":23,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "16\n",
      "0.000125\n",
      "6\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"wvHGn56A9OQVeksOafITvi",
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false
    }
   }
  }
 ],
 "metadata":{
  "kernelspec":{
   "display_name":"Python",
   "language":"python",
   "name":"python"
  },
  "datalore":{
   "version":1,
   "computation_mode":"JUPYTER",
   "package_manager":"pip",
   "base_environment":"default",
   "packages":[
    {
     "name":"nltk",
     "version":"3.7",
     "source":"PIP"
    }
   ]
  }
 },
 "nbformat":4,
 "nbformat_minor":4
}