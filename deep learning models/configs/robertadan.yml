# ->->->->-> Primary <-<-<-<-<-
exp_name: "DAN"
result_dir: "./trained_models"
#num_classes: 1



# ->->->->-> Model <-<-<-<-<-
arch: "robertadan"
vocab_size: 5002
output_dim: 3
embed_dim: 50
hidden_dim: 50
dropout: 0.1


# ->->->->-> Train <-<-<-<-<-
epochs: 20
optimizer: "sgd"
lr: 0.1
lr_schedule: "cosine"
wd: 0.0001
momentum: 0.9

#warmup
warmup_epochs: 10
warmup_lr: 0.0001


# ->->->->-> Dataset <-<-<-<-<-
dataset: "mixed_roberta_label"
batch_size: 16
test_batch_size: 16

#data_dir: "./LSTM_baseline"
#data_fraction: 1.0


# ->->->->-> Misc <-<-<-<-<-
gpu: "0"
no_cuda: False
seed: 11080306
print_freq: 16

# trained_models/DAN/32--lr-0.1_epochs-200_warmuplr-0.0001_warmupepochs-10/tensorboard